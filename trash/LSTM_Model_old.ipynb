{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Josep\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'spacy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_48952/3551153152.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunctional\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mspacy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mspacy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenizer\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mTokenizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mspacy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlang\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0men\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mEnglish\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'spacy'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import spacy\n",
    "from spacy.tokenizer import Tokenizer\n",
    "from spacy.lang.en import English\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datetime import datetime\n",
    "from torch.utils.data import DataLoader\n",
    "from copy import deepcopy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "class vocab_builder:\n",
    "    def __init__(self, tokenized_df):\n",
    "        self.longest= 0\n",
    "        self.idx_word= {}\n",
    "        self.word_idx = {}\n",
    "        self.tracker = {}\n",
    "        tokenized_df['tweet'].apply(vocab_builder.vocab_gen, args=(self,))\n",
    "        self.word_idx['<PAD>'] = len(self.word_idx)\n",
    "        self.idx_word[len(self.idx_word)] = '<PAD>'\n",
    "        self.word_idx['<UNK>'] = len(self.word_idx)\n",
    "        self.idx_word[len(self.idx_word)] = '<UNK>'\n",
    "        \n",
    "    def vocab_gen(sentence, self):\n",
    "        count = 0 \n",
    "        for word in sentence:\n",
    "            count +=1\n",
    "            if word.norm not in self.tracker:\n",
    "                self.tracker[word.norm] = 1\n",
    "                self.word_idx[str(word)] = len(self.word_idx)\n",
    "                self.idx_word[len(self.idx_word)] = word\n",
    "            else:\n",
    "                self.tracker[word.norm] += 1\n",
    "        if self.longest < count:\n",
    "            self.longest = count\n",
    "        return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data_Creator:\n",
    "    def __init__(self, filepath):\n",
    "        data = pd.read_csv(filepath)\n",
    "        data = self.clean(data)\n",
    "        self.tokenized_df = self.tokenize(data)\n",
    "        \n",
    "\n",
    "    def tokenize(self, df):\n",
    "        nlp= spacy.load(\"en_core_web_sm\", disable=[\"parser\"])\n",
    "        tweet = df['tweet'].apply(lambda x: nlp(x.strip()))\n",
    "        tokenized = df.assign(tweet = tweet)\n",
    "        return tokenized\n",
    "    \n",
    "    def clean(self, data):\n",
    "        repl = {'@\\w*': ' ', '&amp;' : 'and','\\su\\s':' you ', '&#\\w*;': ' ', \n",
    "        '#':' ', '\\s2\\s': 'two', 'bihday':\"birthday\", \"ð[^ ]*\": ' ' ,\n",
    "        \"â[^ ]*\": ' ',\"(dont)|(don't)\": 'do not', \"(cant)|(can't)\": \"can not\",\n",
    "        \"(yous)|(you's)\": \"you is\", \"(yous)|(you's)\": \"you is\", \n",
    "        \"(youve)|(you've)\": \"you have\", \"(doesnt)|(doesn't)\": 'does not', \n",
    "        \"(wont)|(won't)\": 'will not', \"[0-9]+\\.*[0-9%]+\\w*\" : \"<NUMBER>\",'\\\\n\\.':' ' ,'\\\\n':' ',\n",
    "        \"\\.{2,}\": '.', \"!{2,}\":'!', \"\\?{2,}\":'?', 'ing[^a-z]':' ', 'ed[^a-z]': ' ', '_':\" \",\n",
    "        ' +': ' '}\n",
    "\n",
    "        cleaned_tweet = data['tweet'].str.lower()\n",
    "        cleaned_tweet = cleaned_tweet.replace(repl, regex=True)\n",
    "        cleaned = data.assign(tweet = cleaned_tweet)\n",
    "        return cleaned.drop(\"Unnamed: 0\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "class data_processor:\n",
    "    def __init__(self, tokenized_df, vocab, test_size, train_size, threshold, most):\n",
    "        self.vocab = vocab\n",
    "        self.most = most\n",
    "        self.threshold = threshold\n",
    "        normalized = tokenized_df['tweet'].apply(data_processor.sentence_normalizer, args=(self,))\n",
    "        normalized_df = tokenized_df.assign(tweet = normalized)\n",
    "        numerized = normalized_df['tweet'].apply(data_processor.numerizer, args=(self,))\n",
    "        numerized_df = normalized_df.assign(numerized_tweet = numerized)\n",
    "        train_valid, self.test = train_test_split(numerized_df, test_size=test_size)\n",
    "        self.train, self.validate = train_test_split(train_valid, train_size=train_size)\n",
    "\n",
    "\n",
    "    def sentence_normalizer(sentence, self):\n",
    "        final_tok = []\n",
    "        count = 0 \n",
    "        for token in sentence:\n",
    "            final_tok.append(token)\n",
    "            count+=1\n",
    "            if count >= self.most:\n",
    "                break\n",
    "        if len(final_tok)<self.most:\n",
    "            final_tok.extend(['<PAD>']*(self.most-len(final_tok)))\n",
    "        return final_tok\n",
    "\n",
    "\n",
    "    def numerizer(x, self):\n",
    "        base = []\n",
    "        for token in x:\n",
    "            try:\n",
    "                if token.norm in self.vocab.tracker:\n",
    "                    if self.vocab.tracker[token.norm]>= self.threshold:\n",
    "                        base.append(self.vocab.word_idx[str(token)])\n",
    "                    else:\n",
    "                        base.append(self.vocab.word_idx['<UNK>'])\n",
    "                else:\n",
    "                    base.append(self.vocab.word_idx['<UNK>'])\n",
    "            except:\n",
    "                base.append(self.vocab.word_idx['<PAD>'])\n",
    "\n",
    "        return base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    def __init__(self, df):\n",
    "        self.features = torch.tensor(np.stack(df['numerized_tweet']))\n",
    "        self.targets = torch.tensor(np.asarray(df['Toxicity'])).unsqueeze(1).float()\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.features.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        features = self.features[idx]\n",
    "        target = self.targets[idx]\n",
    "        return features, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NLP_LSTM(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, layers, dropout, bi):\n",
    "\n",
    "        super(NLP_LSTM, self).__init__()\n",
    "        \n",
    "        ## Params\n",
    "        self.hdim = hidden_dim\n",
    "        self.layers = layers\n",
    "        self.drop = dropout\n",
    "        if bi:\n",
    "            self.bi = 2\n",
    "        else:\n",
    "            self.bi = 1\n",
    "        \n",
    "        \n",
    "        ## layers\n",
    "        self.embedder = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(input_size=embedding_dim, hidden_size=hidden_dim, num_layers=layers, batch_first=True, dropout=dropout, bidirectional = bi)\n",
    "        self.fc1 = nn.Linear(hidden_dim*self.bi, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.drop_layer = nn.Dropout(p=dropout)\n",
    "        \n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \n",
    "        hidden = torch.zeros(layers*self.bi, inputs.shape[0], self.hdim).cuda()\n",
    "        cell = torch.zeros(layers*self.bi, inputs.shape[0], self.hdim).cuda()\n",
    "        \n",
    "        embeddings = self.embedder(inputs)\n",
    "        outputs, (hidden,cell) = self.lstm(embeddings, (hidden,cell))\n",
    "        linear_layer1 = self.relu(self.fc1(outputs[:,-1,:]))\n",
    "        dropper = self.drop_layer(linear_layer1)\n",
    "        linear_layer2 = self.fc2(dropper)\n",
    "        prediction = torch.sigmoid(linear_layer2)\n",
    "        return prediction\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, filepath, batch_size, emded_dim, hidden_dim, learning_rate, epochs, gam, train_size, test_size, early, layers, thresh, most, dropout, bi):\n",
    "\n",
    "        self.gpu_avail = torch.cuda.is_available()\n",
    "        loaded_Data = Data_Creator(filepath)\n",
    "        self.vocabulary = vocab_builder(loaded_Data.tokenized_df)\n",
    "        processed_data = data_processor(loaded_Data.tokenized_df, self.vocabulary, test_size, train_size, thresh, most)\n",
    "        \n",
    "        ## Data\n",
    "        train_data = processed_data.train\n",
    "        vlad_data = processed_data.validate\n",
    "        test_data = processed_data.test\n",
    "\n",
    "        ## Data Loaders\n",
    "\n",
    "        self.train_loader = DataLoader(Dataset(train_data), batch_size=batch_size, shuffle=True)\n",
    "        self.vlad_loader = DataLoader(Dataset(vlad_data), batch_size=batch_size, shuffle=True)\n",
    "        self.test_loader = DataLoader(Dataset(test_data), batch_size=batch_size, shuffle=True)\n",
    "\n",
    "        ## Params\n",
    "        self.last_epoch = epochs\n",
    "        self.early = early\n",
    "        self.bs = batch_size\n",
    "\n",
    "        ## model\n",
    "        if self.gpu_avail:\n",
    "            self.loss = nn.BCELoss().cuda()\n",
    "            self.model = NLP_LSTM(emded_dim, hidden_dim, len(self.vocabulary.word_idx), layers, dropout, bi).cuda().float()\n",
    "            self.best = deepcopy(self.model.state_dict())\n",
    "        else: \n",
    "            self.loss = torch.nn.BCELoss()\n",
    "            self.model = NLP_LSTM(emded_dim, hidden_dim, len(self.vocabulary.word_idx))\n",
    "\n",
    "        ## Optimizer    \n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=learning_rate)\n",
    "        self.lr_scheduler = torch.optim.lr_scheduler.ExponentialLR(self.optimizer, gamma=gam)\n",
    "\n",
    "        ## Loss Tracking\n",
    "        self.min_loss = float('inf')\n",
    "        self.train_loss = []\n",
    "        self.vlad_loss = []\n",
    "        self.test_loss = []\n",
    "\n",
    "    def run(self):\n",
    "        print(\"Training Commencing\")\n",
    "        for epoch in range(0, self.last_epoch):\n",
    "            start_time = datetime.now()\n",
    "            print(F'epoch: {epoch+1}')\n",
    "            print('Training\\n')\n",
    "            self.model.train(True)\n",
    "            self.train()\n",
    "            print('Testing\\n')\n",
    "            self.model.train(False)\n",
    "            self.validation()\n",
    "            if self.vlad_loss[-1] < self.min_loss:\n",
    "                count=0\n",
    "                self.min_loss = self.vlad_loss[-1]\n",
    "                self.best = deepcopy(self.model.state_dict())\n",
    "            if count> self.early:\n",
    "                print('Early Stopping\\n')\n",
    "                break\n",
    "            count += 1\n",
    "            end_time = datetime.now()\n",
    "        self.model.load_state_dict(self.best)\n",
    "        self.model.train(False)\n",
    "        self.test()\n",
    "\n",
    "    \n",
    "    def train(self):\n",
    "        epoch_loss = []\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for text, tag in self.train_loader:\n",
    "            text = text.cuda()\n",
    "            tag = tag.cuda()\n",
    "            self.optimizer.zero_grad()\n",
    "            outputs = self.model.forward(text)\n",
    "            loss = self.loss(outputs, tag)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            epoch_loss.append(loss.item())\n",
    "            classification = torch.round(outputs.squeeze())\n",
    "            num_correct = torch.eq(classification, tag.squeeze()).squeeze()\n",
    "            correct += torch.sum(num_correct)\n",
    "            total += (tag.squeeze()).size(0)\n",
    "\n",
    "        self.lr_scheduler.step()\n",
    "        avg = np.array(epoch_loss).mean()\n",
    "        self.train_loss.append(avg)\n",
    "        acc = correct/total\n",
    "        print(F\"train loss: {avg}\")\n",
    "        print(F\"train accuracy: {acc}\")\n",
    "\n",
    "\n",
    "    def validation(self):\n",
    "        epoch_loss = []\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for text, tag in self.vlad_loader:\n",
    "            text = text.cuda()\n",
    "            tag = tag.cuda()\n",
    "            outputs = self.model.forward(text)\n",
    "            loss = self.loss(outputs, tag)\n",
    "            epoch_loss.append(loss.item())\n",
    "            classification = torch.round(outputs.squeeze())\n",
    "            num_correct = torch.eq(classification, tag.squeeze()).squeeze()\n",
    "            correct += torch.sum(num_correct)\n",
    "            total += (tag.squeeze()).size(0)\n",
    "            \n",
    "        acc = correct/total\n",
    "        avg = np.array(epoch_loss).mean()\n",
    "        self.vlad_loss.append(avg)\n",
    "        print(F\"validation loss: {avg}\")\n",
    "        print(F\"validation accuracy: {acc}\")\n",
    "\n",
    "    def test(self):\n",
    "        epoch_loss = []\n",
    "        correct = 0 \n",
    "        total = 0\n",
    "        \n",
    "        for text, tag in self.test_loader:\n",
    "            text = text.cuda()\n",
    "            tag = tag.cuda()\n",
    "            outputs = self.model.forward(text)\n",
    "            loss = self.loss(outputs, tag)\n",
    "            epoch_loss.append(loss.item())\n",
    "            classification = torch.round(outputs.squeeze())\n",
    "            num_correct = torch.eq(classification, tag.squeeze()).squeeze()\n",
    "            correct += torch.sum(num_correct)\n",
    "            total += (tag.squeeze()).size(0)\n",
    "            \n",
    "        acc = correct/total\n",
    "        avg = np.array(epoch_loss).mean()\n",
    "        self.test_loss.append(avg)\n",
    "        print(F\"test_loss: {avg}\")\n",
    "        print(F\"test accuracy: {acc}\")\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"./data/FinalBalancedDataset.csv\"\n",
    "\n",
    "## HyperParms\n",
    "bs = 50\n",
    "embed = 200\n",
    "hdim = 100\n",
    "lr = 0.00005\n",
    "epochs = 20\n",
    "gam= 0.96\n",
    "tr = 0.9\n",
    "ts= 0.1\n",
    "early = 3\n",
    "layers = 2\n",
    "thresh = 12\n",
    "most = 40\n",
    "dropout = 0.5\n",
    "bi = True\n",
    "\n",
    "h = Trainer(path, bs, embed, hdim, lr, epochs, gam, tr, ts, early, layers, thresh, most, dropout, bi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df =pd.read_csv(\"../data/FinalBalancedDataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Toxicity</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>@user #cnn calls #michigan middle school 'buil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>no comment!  in #australia   #opkillingbay #se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>retweet if you agree!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>23</td>\n",
       "      <td>1</td>\n",
       "      <td>@user @user lumpy says i am a . prove it lumpy.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>34</td>\n",
       "      <td>1</td>\n",
       "      <td>it's unbelievable that in the 21st century we'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56739</th>\n",
       "      <td>56739</td>\n",
       "      <td>1</td>\n",
       "      <td>you're such a retard i hope you get type 2 dia...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56740</th>\n",
       "      <td>56740</td>\n",
       "      <td>1</td>\n",
       "      <td>you's a muthaf***in lie &amp;#8220;@LifeAsKing: @2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56741</th>\n",
       "      <td>56741</td>\n",
       "      <td>1</td>\n",
       "      <td>you've gone and broke the wrong heart baby, an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56742</th>\n",
       "      <td>56742</td>\n",
       "      <td>1</td>\n",
       "      <td>young buck wanna eat!!.. dat nigguh like I ain...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56743</th>\n",
       "      <td>56743</td>\n",
       "      <td>1</td>\n",
       "      <td>youu got wild bitches tellin you lies</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>24153 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0  Toxicity                                              tweet\n",
       "13             13         1  @user #cnn calls #michigan middle school 'buil...\n",
       "14             14         1  no comment!  in #australia   #opkillingbay #se...\n",
       "17             17         1                             retweet if you agree! \n",
       "23             23         1    @user @user lumpy says i am a . prove it lumpy.\n",
       "34             34         1  it's unbelievable that in the 21st century we'...\n",
       "...           ...       ...                                                ...\n",
       "56739       56739         1  you're such a retard i hope you get type 2 dia...\n",
       "56740       56740         1  you's a muthaf***in lie &#8220;@LifeAsKing: @2...\n",
       "56741       56741         1  you've gone and broke the wrong heart baby, an...\n",
       "56742       56742         1  young buck wanna eat!!.. dat nigguh like I ain...\n",
       "56743       56743         1              youu got wild bitches tellin you lies\n",
       "\n",
       "[24153 rows x 3 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['Toxicity']==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "470412a98dbcf49b8d099763d9ca7a4a894f05babda629466c75320fdfbb3b4a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
