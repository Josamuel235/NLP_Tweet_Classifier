{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import spacy\n",
    "from spacy.tokenizer import Tokenizer\n",
    "from spacy.lang.en import English\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datetime import datetime\n",
    "from torch.utils.data import DataLoader\n",
    "from copy import deepcopy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "class vocab_builder:\n",
    "    def __init__(self, tokenized_df):\n",
    "        self.longest= 0\n",
    "        self.idx_word= {}\n",
    "        self.word_idx = {}\n",
    "        self.tracker = {}\n",
    "        tokenized_df['tweet'].apply(vocab_builder.vocab_gen, args=(self,))\n",
    "        self.word_idx['<PAD>'] = len(self.word_idx)\n",
    "        self.idx_word[len(self.idx_word)] = '<PAD>'\n",
    "        self.word_idx['<UNK>'] = len(self.word_idx)\n",
    "        self.idx_word[len(self.idx_word)] = '<UNK>'\n",
    "        \n",
    "    def vocab_gen(sentence, self):\n",
    "        count = 0 \n",
    "        for word in sentence:\n",
    "            count +=1\n",
    "            if word.norm not in self.tracker:\n",
    "                self.tracker[word.norm] = 1\n",
    "                self.word_idx[str(word)] = len(self.word_idx)\n",
    "                self.idx_word[len(self.idx_word)] = word\n",
    "            else:\n",
    "                self.tracker[word.norm] += 1\n",
    "        if self.longest < count:\n",
    "            self.longest = count\n",
    "        return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data_Creator:\n",
    "    def __init__(self, filepath):\n",
    "        data = pd.read_csv(filepath)\n",
    "        data = self.clean(data)\n",
    "        self.tokenized_df = self.tokenize(data)\n",
    "        \n",
    "\n",
    "    def tokenize(self, df):\n",
    "        nlp= spacy.load(\"en_core_web_sm\", disable=[\"parser\"])\n",
    "        tweet = df['tweet'].apply(lambda x: nlp(x.strip()))\n",
    "        tokenized = df.assign(tweet = tweet)\n",
    "        return tokenized\n",
    "    \n",
    "    def clean(self, data):\n",
    "        repl = {'@\\w*': ' ', '&amp;' : 'and','\\su\\s':' you ', '&#\\w*;': ' ', \n",
    "        '#':' ', '\\s2\\s': 'two', 'bihday':\"birthday\", \"ð[^ ]*\": ' ' ,\n",
    "        \"â[^ ]*\": ' ',\"(dont)|(don't)\": 'do not', \"(cant)|(can't)\": \"can not\",\n",
    "        \"(yous)|(you's)\": \"you is\", \"(yous)|(you's)\": \"you is\", \n",
    "        \"(youve)|(you've)\": \"you have\", \"(doesnt)|(doesn't)\": 'does not', \n",
    "        \"(wont)|(won't)\": 'will not', \"[0-9]+\\.*[0-9%]+\\w*\" : \"<NUMBER>\",'\\\\n\\.':' ' ,'\\\\n':' ',\n",
    "        \"\\.{2,}\": '.', \"!{2,}\":'!', \"\\?{2,}\":'?', 'ing[^a-z]':' ', 'ed[^a-z]': ' ', '_':\" \",\n",
    "        ' +': ' '}\n",
    "\n",
    "        cleaned_tweet = data['tweet'].str.lower()\n",
    "        cleaned_tweet = cleaned_tweet.replace(repl, regex=True)\n",
    "        cleaned = data.assign(tweet = cleaned_tweet)\n",
    "        return cleaned.drop(\"Unnamed: 0\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "class data_processor:\n",
    "    def __init__(self, tokenized_df, vocab, test_size, train_size, threshold, most):\n",
    "        self.vocab = vocab\n",
    "        self.most = most\n",
    "        self.threshold = threshold\n",
    "        normalized = tokenized_df['tweet'].apply(data_processor.sentence_normalizer, args=(self,))\n",
    "        normalized_df = tokenized_df.assign(tweet = normalized)\n",
    "        numerized = normalized_df['tweet'].apply(data_processor.numerizer, args=(self,))\n",
    "        numerized_df = normalized_df.assign(numerized_tweet = numerized)\n",
    "        train_valid, self.test = train_test_split(numerized_df, test_size=test_size)\n",
    "        self.train, self.validate = train_test_split(train_valid, train_size=train_size)\n",
    "\n",
    "\n",
    "    def sentence_normalizer(sentence, self):\n",
    "        final_tok = []\n",
    "        count = 0 \n",
    "        for token in sentence:\n",
    "            final_tok.append(token)\n",
    "            count+=1\n",
    "            if count >= self.most:\n",
    "                break\n",
    "        if len(final_tok)<self.most:\n",
    "            final_tok.extend(['<PAD>']*(self.most-len(final_tok)))\n",
    "        return final_tok\n",
    "\n",
    "\n",
    "    def numerizer(x, self):\n",
    "        base = []\n",
    "        for token in x:\n",
    "            try:\n",
    "                if token.norm in self.vocab.tracker:\n",
    "                    if self.vocab.tracker[token.norm]>= self.threshold:\n",
    "                        base.append(self.vocab.word_idx[str(token)])\n",
    "                    else:\n",
    "                        base.append(self.vocab.word_idx['<UNK>'])\n",
    "                else:\n",
    "                    base.append(self.vocab.word_idx['<UNK>'])\n",
    "            except:\n",
    "                base.append(self.vocab.word_idx['<PAD>'])\n",
    "\n",
    "        return base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    def __init__(self, df):\n",
    "        self.features = torch.tensor(np.stack(df['numerized_tweet']))\n",
    "        self.targets = torch.tensor(np.asarray(df['Toxicity'])).unsqueeze(1).float()\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.features.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        features = self.features[idx]\n",
    "        target = self.targets[idx]\n",
    "        return features, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NLP_LSTM(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, layers, dropout, bi):\n",
    "\n",
    "        super(NLP_LSTM, self).__init__()\n",
    "        \n",
    "        ## Params\n",
    "        self.hdim = hidden_dim\n",
    "        self.layers = layers\n",
    "        self.drop = dropout\n",
    "        if bi:\n",
    "            self.bi = 2\n",
    "        else:\n",
    "            self.bi = 1\n",
    "        \n",
    "        \n",
    "        ## layers\n",
    "        self.embedder = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(input_size=embedding_dim, hidden_size=hidden_dim, num_layers=layers, batch_first=True, dropout=dropout, bidirectional = bi)\n",
    "        self.fc1 = nn.Linear(hidden_dim*self.bi, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.drop_layer = nn.Dropout(p=dropout)\n",
    "        \n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \n",
    "        hidden = torch.zeros(layers*self.bi, inputs.shape[0], self.hdim).cuda()\n",
    "        cell = torch.zeros(layers*self.bi, inputs.shape[0], self.hdim).cuda()\n",
    "        \n",
    "        embeddings = self.embedder(inputs)\n",
    "        outputs, (hidden,cell) = self.lstm(embeddings, (hidden,cell))\n",
    "        linear_layer1 = self.relu(self.fc1(outputs[:,-1,:]))\n",
    "        dropper = self.drop_layer(linear_layer1)\n",
    "        linear_layer2 = self.fc2(dropper)\n",
    "        prediction = torch.sigmoid(linear_layer2)\n",
    "        return prediction\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, filepath, batch_size, emded_dim, hidden_dim, learning_rate, epochs, gam, train_size, test_size, early, layers, thresh, most, dropout, bi):\n",
    "\n",
    "        self.gpu_avail = torch.cuda.is_available()\n",
    "        loaded_Data = Data_Creator(filepath)\n",
    "        self.vocabulary = vocab_builder(loaded_Data.tokenized_df)\n",
    "        processed_data = data_processor(loaded_Data.tokenized_df, self.vocabulary, test_size, train_size, thresh, most)\n",
    "        \n",
    "        ## Data\n",
    "        train_data = processed_data.train\n",
    "        vlad_data = processed_data.validate\n",
    "        test_data = processed_data.test\n",
    "\n",
    "        ## Data Loaders\n",
    "\n",
    "        self.train_loader = DataLoader(Dataset(train_data), batch_size=batch_size, shuffle=True)\n",
    "        self.vlad_loader = DataLoader(Dataset(vlad_data), batch_size=batch_size, shuffle=True)\n",
    "        self.test_loader = DataLoader(Dataset(test_data), batch_size=batch_size, shuffle=True)\n",
    "\n",
    "        ## Params\n",
    "        self.last_epoch = epochs\n",
    "        self.early = early\n",
    "        self.bs = batch_size\n",
    "\n",
    "        ## model\n",
    "        if self.gpu_avail:\n",
    "            self.loss = nn.BCELoss().cuda()\n",
    "            self.model = NLP_LSTM(emded_dim, hidden_dim, len(self.vocabulary.word_idx), layers, dropout, bi).cuda().float()\n",
    "            self.best = deepcopy(self.model.state_dict())\n",
    "        else: \n",
    "            self.loss = torch.nn.BCELoss()\n",
    "            self.model = NLP_LSTM(emded_dim, hidden_dim, len(self.vocabulary.word_idx))\n",
    "\n",
    "        ## Optimizer    \n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=learning_rate)\n",
    "        self.lr_scheduler = torch.optim.lr_scheduler.ExponentialLR(self.optimizer, gamma=gam)\n",
    "\n",
    "        ## Loss Tracking\n",
    "        self.min_loss = float('inf')\n",
    "        self.train_loss = []\n",
    "        self.vlad_loss = []\n",
    "        self.test_loss = []\n",
    "\n",
    "    def run(self):\n",
    "        print(\"Training Commencing\")\n",
    "        for epoch in range(0, self.last_epoch):\n",
    "            start_time = datetime.now()\n",
    "            print(F'epoch: {epoch+1}')\n",
    "            print('Training\\n')\n",
    "            self.model.train(True)\n",
    "            self.train()\n",
    "            print('Testing\\n')\n",
    "            self.model.train(False)\n",
    "            self.validation()\n",
    "            if self.vlad_loss[-1] < self.min_loss:\n",
    "                count=0\n",
    "                self.min_loss = self.vlad_loss[-1]\n",
    "                self.best = deepcopy(self.model.state_dict())\n",
    "            if count> self.early:\n",
    "                print('Early Stopping\\n')\n",
    "                break\n",
    "            count += 1\n",
    "            end_time = datetime.now()\n",
    "        self.model.load_state_dict(self.best)\n",
    "        self.model.train(False)\n",
    "        self.test()\n",
    "\n",
    "    \n",
    "    def train(self):\n",
    "        epoch_loss = []\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for text, tag in self.train_loader:\n",
    "            text = text.cuda()\n",
    "            tag = tag.cuda()\n",
    "            self.optimizer.zero_grad()\n",
    "            outputs = self.model.forward(text)\n",
    "            loss = self.loss(outputs, tag)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            epoch_loss.append(loss.item())\n",
    "            classification = torch.round(outputs.squeeze())\n",
    "            num_correct = torch.eq(classification, tag.squeeze()).squeeze()\n",
    "            correct += torch.sum(num_correct)\n",
    "            total += (tag.squeeze()).size(0)\n",
    "\n",
    "        self.lr_scheduler.step()\n",
    "        avg = np.array(epoch_loss).mean()\n",
    "        self.train_loss.append(avg)\n",
    "        acc = correct/total\n",
    "        print(F\"train loss: {avg}\")\n",
    "        print(F\"train accuracy: {acc}\")\n",
    "\n",
    "\n",
    "    def validation(self):\n",
    "        epoch_loss = []\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for text, tag in self.vlad_loader:\n",
    "            text = text.cuda()\n",
    "            tag = tag.cuda()\n",
    "            outputs = self.model.forward(text)\n",
    "            loss = self.loss(outputs, tag)\n",
    "            epoch_loss.append(loss.item())\n",
    "            classification = torch.round(outputs.squeeze())\n",
    "            num_correct = torch.eq(classification, tag.squeeze()).squeeze()\n",
    "            correct += torch.sum(num_correct)\n",
    "            total += (tag.squeeze()).size(0)\n",
    "            \n",
    "        acc = correct/total\n",
    "        avg = np.array(epoch_loss).mean()\n",
    "        self.vlad_loss.append(avg)\n",
    "        print(F\"validation loss: {avg}\")\n",
    "        print(F\"validation accuracy: {acc}\")\n",
    "\n",
    "    def test(self):\n",
    "        epoch_loss = []\n",
    "        correct = 0 \n",
    "        total = 0\n",
    "        \n",
    "        for text, tag in self.test_loader:\n",
    "            text = text.cuda()\n",
    "            tag = tag.cuda()\n",
    "            outputs = self.model.forward(text)\n",
    "            loss = self.loss(outputs, tag)\n",
    "            epoch_loss.append(loss.item())\n",
    "            classification = torch.round(outputs.squeeze())\n",
    "            num_correct = torch.eq(classification, tag.squeeze()).squeeze()\n",
    "            correct += torch.sum(num_correct)\n",
    "            total += (tag.squeeze()).size(0)\n",
    "            \n",
    "        acc = correct/total\n",
    "        avg = np.array(epoch_loss).mean()\n",
    "        self.test_loss.append(avg)\n",
    "        print(F\"test_loss: {avg}\")\n",
    "        print(F\"test accuracy: {acc}\")\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"./data/FinalBalancedDataset.csv\"\n",
    "\n",
    "## HyperParms\n",
    "bs = 50\n",
    "embed = 200\n",
    "hdim = 100\n",
    "lr = 0.00005\n",
    "epochs = 20\n",
    "gam= 0.96\n",
    "tr = 0.9\n",
    "ts= 0.1\n",
    "early = 3\n",
    "layers = 2\n",
    "thresh = 12\n",
    "most = 40\n",
    "dropout = 0.5\n",
    "bi = True\n",
    "\n",
    "h = Trainer(path, bs, embed, hdim, lr, epochs, gam, tr, ts, early, layers, thresh, most, dropout, bi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        nlp= spacy.load(\"en_core_web_sm\", disable=[\"parser\"])\n",
    "        tweet = df['tweet'].apply(lambda x: nlp(x.strip()))\n",
    "\n",
    "        nlp = spacy.load(\"en_core_web_sm\")\n",
    "        contextualSpellCheck.add_to_pipe(nlp)\n",
    "        tweet = df['tweet'].apply(lambda x: nlp(nlp(x.strip())._.outcome_spellCheck))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_dollar_delete_equal(arr):\n",
    "    # fill in\n",
    "    \n",
    "    \n",
    "    for i in range(len(arr)):\n",
    "      \n",
    "      skip = 0\n",
    "      curr = []\n",
    "      for j in range(len(arr[i])-1, -1, -1):\n",
    "        if skip>0 and not arr[i][j] == \"$\":\n",
    "          skip -=1\n",
    "          continue\n",
    "        if arr[i][j] == \"$\":\n",
    "          skip+=1\n",
    "        else:\n",
    "          curr.append(arr[i][j])\n",
    "      if  i==0:\n",
    "        base = curr\n",
    "      else:\n",
    "        print(curr)\n",
    "        print(base)\n",
    "        if base != curr:\n",
    "          return False\n",
    "\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['e', 's', 'c', '0', '9', 'a']\n",
      "['e', 's', '9', 'a']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_dollar_delete_equal([\"a90$n$c$se\", \"a90n$cse\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "470412a98dbcf49b8d099763d9ca7a4a894f05babda629466c75320fdfbb3b4a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
