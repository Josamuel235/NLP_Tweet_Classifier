{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from spacy.tokenizer import Tokenizer\n",
    "from spacy.lang.en import English\n",
    "from sklearn.model_selection import train_test_split\n",
    "import datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "class vocab_builder:\n",
    "    def __init__(self, tokenized_df):\n",
    "        self.idx_word= {}\n",
    "        self.longest= 0\n",
    "        self.word_idx = {}\n",
    "        tokenized_df['tweet'].apply(vocab_builder.word_to_idx, args=(self,))\n",
    "        tokenized_df['tweet'].apply(vocab_builder.idx_to_word, args=(self,))\n",
    "        self.idx_word[99999] = '<PAD>'\n",
    "        self.word_idx['<PAD>'] = 99999\n",
    "        \n",
    "    def word_to_idx(sentence, self):\n",
    "        count = 0 \n",
    "        for word in sentence:\n",
    "            count +=1\n",
    "            if word.norm not in self.idx_word:\n",
    "                #word_idx[word] = len(word_idx)\n",
    "                self.idx_word[len(self.idx_word)] = word\n",
    "        if self.longest < count:\n",
    "            self.longest = count\n",
    "        return \n",
    "\n",
    "    def idx_to_word(sentence, self):\n",
    "        for word in sentence:\n",
    "            if word not in self.word_idx:\n",
    "                #word_idx[word] = len(word_idx)\n",
    "                self.word_idx[word] = len(self.word_idx)\n",
    "        return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    def __init__(self, filepath):\n",
    "        self.data = pd.read_csv(filepath)\n",
    "        self.tokenized_df = Dataset.tokenize(self, self.data)\n",
    "        \n",
    "\n",
    "    def tokenize(self, df):\n",
    "        nlp= English()\n",
    "        token_gen = Tokenizer(nlp.vocab)\n",
    "        tweet = df['tweet'].apply(lambda x: token_gen(x))\n",
    "        tokenized = df.assign(tweet = tweet)\n",
    "        return tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "class data_processor:\n",
    "    def __init__(self, tokenized_df, vocab):\n",
    "        self.vocab = vocab\n",
    "        self.normalized = tokenized_df['tweet'].apply(data_processor.sentence_normalizer, args=(self,))\n",
    "        self.normalized_df = tokenized_df.assign(tweet = self.normalized)\n",
    "        self.normalized_df = self.normalized_df.drop(\"Unnamed: 0\" , axis=1)\n",
    "        self.numerized = self.normalized_df['tweet'].apply(data_processor.numerizer, args=(self,))\n",
    "        self.numerized_df = self.normalized_df.assign(numerized_tweet = self.numerized)\n",
    "\n",
    "\n",
    "    def sentence_normalizer(sentence, self):\n",
    "        final_tok = []\n",
    "        for token in sentence:\n",
    "            final_tok.append(token)\n",
    "        if len(final_tok)<self.vocab.longest:\n",
    "            final_tok.extend(['<PAD>']*(self.vocab.longest-len(final_tok)))\n",
    "        return final_tok\n",
    "\n",
    "\n",
    "    def numerizer(x, self):\n",
    "        base = []\n",
    "        for token in x:\n",
    "            try:\n",
    "                base.append(self.vocab.word_idx[token])\n",
    "            except:\n",
    "                base.append(99999)\n",
    "        return base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data_Loader:\n",
    "    def __init__(self, df, train_size, val_size, test_size, gpu):\n",
    "        dataframe = df[['Toxicity',\"numerized_tweet\"]]\n",
    "        self.train_valid, self.test = train_test_split(dataframe, test_size=test_size)\n",
    "        self.train, self.validate = train_test_split(dataframe, test_size=val_size)\n",
    "        self.test = Data_Loader.convert(self.test, gpu)\n",
    "        self.validate = Data_Loader.convert(self.validate, gpu)\n",
    "        self.train = Data_Loader.convert(self.train, gpu)\n",
    "\n",
    "    \n",
    "    def convert(self, data_partition, gpu_cpu):\n",
    "        if gpu_cpu:\n",
    "            text = torch.tensor(np.stack(data_partition['tweet']))\n",
    "            tag = torch.tensor(data_partition['Toxicity']).unsqueeze(1)\n",
    "            return torch.cat((text,tag),1).cuda()\n",
    "            \n",
    "        else:\n",
    "            text = np.stack(data_partition['tweet'])\n",
    "            tag = np.array(data_partition['Toxicity'])\n",
    "            tag = np.expand_dims(tag, 1)\n",
    "            return np.concatenate((text,tag),1)\n",
    "\n",
    "    def shuffle(self, gpu_cpu):\n",
    "        if gpu_cpu:\n",
    "            idx =torch.randperm(self.train.shape[0])\n",
    "            self.train = self.train[idx]\n",
    "        else: \n",
    "            np.random.shuffle(self.train)  \n",
    "\n",
    "    def create_batches(self, data, batch_size):\n",
    "        length = len(data)\n",
    "        base=[]\n",
    "        tracker = 0 \n",
    "        while tracker<len(length):\n",
    "            if (tracker+64)>=len(length):\n",
    "                base.append((tracker,len(length)))\n",
    "                break\n",
    "            base.append((tracker,tracker+64))\n",
    "            tracker+=64\n",
    "        \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NLP_LSTM(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size):\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "        #self.fc = nn.Linear(self.hidden_size, self.vocab.idx)\n",
    "        self.embedder = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(input_size=embedding_dim, hidden_size=hidden_dim, num_layers=2, batch_first=True)\n",
    "        \n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embeddings = self.embedder(inputs)\n",
    "        outputs, hidden_states = self.lstm(embeddings, hidden_states)\n",
    "        linear_layer1 = torch.relu(self.fc(outputs))\n",
    "        linear_layer2 = torch.relu(self.fc(linear_layer1))\n",
    "        prediction = F.sigmoid(linear_layer2)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, filepath, batch_size, learning_rate, epochs):\n",
    "        self.last_epoch = epochs\n",
    "        self.gpu_avail = torch.cuda.is_available()\n",
    "        self.loaded_Data = Dataset(filepath)\n",
    "        self.vocabulary = vocab_builder(self.loaded_Data.tokenized_df)\n",
    "        self.processed_data = data_processor(self.loaded_Data.tokenized_df,self.vocabulary)\n",
    "        \n",
    "\n",
    "        ## model\n",
    "        self.model = NLP_LSTM()\n",
    "        self.lr = learning_rate\n",
    "        self.batch_size= batch_size\n",
    "        self.loss = torch.nn.BCELoss()\n",
    "        self.__optimizer = torch.optim.Adam(self.model.parameters(), lr=0.001)\n",
    "        self.lr_scheduler = torch.optim.lr_scheduler.ExponentialLR(self.__optimizer, gamma=0.9)\n",
    "\n",
    "    def main(self):\n",
    "        print(\"Training Commencing\")\n",
    "        for epoch in range(0, self.last_epoch):\n",
    "            start_time = datetime.now()\n",
    "            print(F'epoch: {epoch+1}')\n",
    "            \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = Trainer(\"C:/MyJoseph/Projects/NLP_text_ML/data/FinalBalancedDataset.csv\", 64, 0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "twitterscrape",
   "language": "python",
   "name": "twitterscrape"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "470412a98dbcf49b8d099763d9ca7a4a894f05babda629466c75320fdfbb3b4a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
