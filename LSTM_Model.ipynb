{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\MyJoseph\\Projects\\Scraper\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import spacy\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datetime import datetime\n",
    "from torch.utils.data import DataLoader\n",
    "from copy import deepcopy\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class vocab_builder:\n",
    "    def __init__(self, tokenized_df):\n",
    "        self.longest= 0\n",
    "        self.idx_word= {}\n",
    "        self.word_idx = {}\n",
    "        self.tracker = {}\n",
    "        tokenized_df['tweet'].apply(vocab_builder.vocab_gen, args=(self,))\n",
    "        self.word_idx['<PAD>'] = len(self.word_idx)\n",
    "        self.idx_word[len(self.idx_word)] = '<PAD>'\n",
    "        self.word_idx['<UNK>'] = len(self.word_idx)\n",
    "        self.idx_word[len(self.idx_word)] = '<UNK>'\n",
    "        \n",
    "    def vocab_gen(sentence, self):\n",
    "        count = 0 \n",
    "        for word in sentence:\n",
    "            count +=1\n",
    "            if word.norm not in self.tracker:\n",
    "                self.tracker[word.norm] = 1\n",
    "                self.word_idx[str(word)] = len(self.word_idx)\n",
    "                self.idx_word[len(self.idx_word)] = str(word)\n",
    "            else:\n",
    "                self.tracker[word.norm] += 1\n",
    "        if self.longest < count:\n",
    "            self.longest = count\n",
    "        return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data_Creator:\n",
    "    def __init__(self, filepath):\n",
    "        data = pd.read_csv(filepath)\n",
    "        data = self.clean(data)\n",
    "        self.tokenized_df = self.tokenize(data)\n",
    "        \n",
    "\n",
    "    def tokenize(self, df):\n",
    "        nlp = spacy.load(\"en_core_web_sm\")\n",
    "        tweet = df['tweet'].apply(lambda x: nlp(x.strip()))\n",
    "        tokenized = df.assign(tweet = tweet)\n",
    "        return tokenized\n",
    "    \n",
    "    def clean(self, data):\n",
    "        \n",
    "        repl = {'@\\w*': ' ', '&amp;' : 'and','\\su\\s':' you ', '&#\\w*;': ' ', \n",
    "        '#':' ', '\\s2\\s': 'two', 'bihday':\"birthday\", \"ð[^ ]*\": ' ' ,\n",
    "        \"â[^ ]*\": ' ',\"(dont)|(don't)\": 'do not', \"(cant)|(can't)\": \"can not\",\n",
    "        \"(yous)|(you's)\": \"you is\", \"(yous)|(you's)\": \"you is\", \n",
    "        \"(youve)|(you've)\": \"you have\", \"(doesnt)|(doesn't)\": 'does not', \n",
    "        \"(wont)|(won't)\": 'will not', \"[0-9]+\\.*[0-9%]+\\w*\" : \"NUMBER\",'\\\\n\\.':' ' ,'\\\\n':' ',\n",
    "        \"\\.{2,}\": '.', \"!{2,}\":'!', \"\\?{2,}\":'?', 'ing[^a-z]':' ', 'ed[^a-z]': ' ', '_':\" \",\n",
    "        ' +': ' '}\n",
    "\n",
    "        cleaned_tweet = data['tweet'].str.lower()\n",
    "        cleaned_tweet = cleaned_tweet.replace(repl, regex=True)\n",
    "        cleaned = data.assign(tweet = cleaned_tweet)\n",
    "        return cleaned.drop(\"Unnamed: 0\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class data_processor:\n",
    "    def __init__(self, tokenized_df, vocab, test_size, train_size, threshold, most):\n",
    "        self.vocab = vocab\n",
    "        self.most = most\n",
    "        self.threshold = threshold\n",
    "        normalized = tokenized_df['tweet'].apply(data_processor.sentence_normalizer, args=(self,))\n",
    "        normalized_df = tokenized_df.assign(tweet = normalized)\n",
    "        numerized = normalized_df['tweet'].apply(data_processor.numerizer, args=(self,))\n",
    "        numerized_df = normalized_df.assign(numerized_tweet = numerized)\n",
    "        train_valid, self.test = train_test_split(numerized_df, test_size=test_size)\n",
    "        self.train, self.validate = train_test_split(train_valid, train_size=train_size)\n",
    "\n",
    "\n",
    "    def sentence_normalizer(sentence, self):\n",
    "        final_tok = []\n",
    "        count = 0 \n",
    "        for token in sentence:\n",
    "            final_tok.append(token)\n",
    "            count+=1\n",
    "            if count >= self.most:\n",
    "                break\n",
    "        if len(final_tok)<self.most:\n",
    "            final_tok.extend(['<PAD>']*(self.most-len(final_tok)))\n",
    "        return final_tok\n",
    "\n",
    "\n",
    "    def numerizer(x, self):\n",
    "        base = []\n",
    "        for token in x:\n",
    "            try:\n",
    "                if token.norm in self.vocab.tracker:\n",
    "                    if self.vocab.tracker[token.norm]>= self.threshold:\n",
    "                        base.append(self.vocab.word_idx[str(token)])\n",
    "                    else:\n",
    "                        base.append(self.vocab.word_idx['<UNK>'])\n",
    "                else:\n",
    "                    base.append(self.vocab.word_idx['<UNK>'])\n",
    "            except:\n",
    "                base.append(self.vocab.word_idx['<PAD>'])\n",
    "\n",
    "        return base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    def __init__(self, df):\n",
    "        self.features = torch.tensor(np.stack(df['numerized_tweet']))\n",
    "        self.targets = torch.tensor(np.asarray(df['Toxicity'])).unsqueeze(1).float()\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.features.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        features = self.features[idx]\n",
    "        target = self.targets[idx]\n",
    "        return features, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NLP_LSTM(nn.Module):\n",
    "    def __init__(self, gpu, embedding_dim, hidden_dim, vocab_size, layers, dropout, bi):\n",
    "\n",
    "        super(NLP_LSTM, self).__init__()\n",
    "        \n",
    "        # GPU\n",
    "        self.gpu = gpu\n",
    "        \n",
    "        ## Params\n",
    "        self.hdim = hidden_dim\n",
    "        self.layers = layers\n",
    "        self.drop = dropout\n",
    "        if bi:\n",
    "            self.bi = 2\n",
    "        else:\n",
    "            self.bi = 1\n",
    "        \n",
    "        \n",
    "        ## layers\n",
    "        self.embedder = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.fc0 = nn.Linear(embedding_dim, embedding_dim)\n",
    "        self.lstm = nn.LSTM(input_size=embedding_dim, hidden_size=hidden_dim, num_layers=layers, batch_first=True, dropout=dropout, bidirectional = bi)\n",
    "        self.fc1 = nn.Linear(hidden_dim*self.bi, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.drop_layer = nn.Dropout(p=dropout)\n",
    "        \n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \n",
    "        if self.gpu:\n",
    "            hidden = torch.zeros(self.layers*self.bi, inputs.shape[0], self.hdim).cuda()\n",
    "            cell = torch.zeros(self.layers*self.bi, inputs.shape[0], self.hdim).cuda()\n",
    "        else:\n",
    "            hidden = torch.zeros(self.layers*self.bi, inputs.shape[0], self.hdim)\n",
    "            cell = torch.zeros(self.layers*self.bi, inputs.shape[0], self.hdim)\n",
    "        \n",
    "        embeddings = self.embedder(inputs)\n",
    "        linear_layer1 = self.relu(self.fc0(embeddings))\n",
    "        linear_layer2 = self.relu(self.fc0(linear_layer1))\n",
    "        dropper1 = self.drop_layer(linear_layer2)\n",
    "        outputs, (hidden,cell) = self.lstm(dropper1, (hidden,cell))\n",
    "        linear_layer3 = self.relu(self.fc1(outputs[:,-1,:]))\n",
    "        dropper2 = self.drop_layer(linear_layer3)\n",
    "        linear_layer4 = self.fc2(dropper2)\n",
    "        prediction = torch.sigmoid(linear_layer4)\n",
    "        return prediction\n",
    "        \n",
    "    def tester(self, inputs):\n",
    "        \n",
    "        if self.gpu:\n",
    "            hidden = torch.zeros(self.layers*self.bi, inputs.shape[0], self.hdim).cuda()\n",
    "            cell = torch.zeros(self.layers*self.bi, inputs.shape[0], self.hdim).cuda()\n",
    "        else:\n",
    "            hidden = torch.zeros(self.layers*self.bi, inputs.shape[0], self.hdim)\n",
    "            cell = torch.zeros(self.layers*self.bi, inputs.shape[0], self.hdim)\n",
    "        \n",
    "        embeddings = self.embedder(inputs)\n",
    "        x = self.relu(self.fc0(embeddings))\n",
    "        y = self.relu(self.fc0(x))\n",
    "        outputs, (hidden,cell) = self.lstm(y, (hidden,cell))\n",
    "        linear_layer1 = self.relu(self.fc1(outputs[:,-1,:]))\n",
    "        linear_layer2 = self.fc2(linear_layer1)\n",
    "        prediction = torch.sigmoid(linear_layer2)\n",
    "        return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, filepath, models, batch_size, emded_dim, hidden_dim, learning_rate, epochs, gam, train_size, test_size, early, layers, thresh, most, dropout, bi):\n",
    "\n",
    "        self.gpu_avail = torch.cuda.is_available()\n",
    "        loaded_Data = Data_Creator(filepath)\n",
    "        self.vocabulary = vocab_builder(loaded_Data.tokenized_df)\n",
    "        processed_data = data_processor(loaded_Data.tokenized_df, self.vocabulary, test_size, train_size, thresh, most)\n",
    "        \n",
    "        ## Data\n",
    "        self.train_data = processed_data.train\n",
    "        self.vlad_data = processed_data.validate\n",
    "        self.test_data = processed_data.test\n",
    "\n",
    "        ## Data Loaders\n",
    "        self.train_loader = DataLoader(Dataset(self.train_data), batch_size=batch_size, shuffle=True)\n",
    "        self.vlad_loader = DataLoader(Dataset(self.vlad_data), batch_size=batch_size, shuffle=True)\n",
    "        self.test_loader = DataLoader(Dataset(self.test_data), batch_size=batch_size, shuffle=True)\n",
    "\n",
    "        ## Models\n",
    "        self.models = models\n",
    "\n",
    "        ## Params\n",
    "        self.last_epoch = epochs\n",
    "        self.early = early\n",
    "        self.bs = batch_size\n",
    "\n",
    "        ## model\n",
    "        if self.gpu_avail:\n",
    "            self.loss = nn.BCELoss().cuda()\n",
    "            self.model = NLP_LSTM(self.gpu_avail, emded_dim, hidden_dim, len(self.vocabulary.word_idx), layers, dropout, bi).cuda().float()\n",
    "            self.best = deepcopy(self.model.state_dict())\n",
    "        else: \n",
    "            self.loss = torch.nn.BCELoss()\n",
    "            self.model = NLP_LSTM(self.gpu_avail, emded_dim, hidden_dim, len(self.vocabulary.word_idx), layers, dropout, bi)\n",
    "\n",
    "        ## Optimizer    \n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=learning_rate)\n",
    "        self.lr_scheduler = torch.optim.lr_scheduler.ExponentialLR(self.optimizer, gamma=gam)\n",
    "\n",
    "        ## Stat Tracking\n",
    "        self.min_loss = float('inf')\n",
    "        \n",
    "        self.train_loss = []\n",
    "        self.vlad_loss = []\n",
    "        \n",
    "        self.train_acc = []\n",
    "        self.vlad_acc = []\n",
    "\n",
    "    def run(self):\n",
    "        if 'simple' in self.models:\n",
    "            simple = self.Simple_gusser_Model()\n",
    "            print('The Simple Evaluation Metrics:\\n')\n",
    "            print(simple)\n",
    "            print(\"\\n\")\n",
    "        if 'svm' in self.models:\n",
    "            svm = self.svm_model()\n",
    "            print('The SVM Evaluation Metrics:\\n')\n",
    "            print(svm)\n",
    "            print(\"\\n\")\n",
    "        if 'lstm' in self.models:\n",
    "            lstm = self.run_LSTM()\n",
    "            print('The LSTM Evaluation Metrics:\\n')\n",
    "            print(lstm)\n",
    "            print(\"\\n\")\n",
    "\n",
    "\n",
    "    def run_LSTM(self):\n",
    "        print(\"Training Commencing\")\n",
    "        start_time = datetime.now()\n",
    "        for epoch in range(0, self.last_epoch):\n",
    "            print(F'epoch: {epoch+1}')\n",
    "            print('Training\\n')\n",
    "            self.model.train(True)\n",
    "            self.train()\n",
    "            print('Testing\\n')\n",
    "            self.model.train(False)\n",
    "            self.validation()\n",
    "            if self.vlad_loss[-1] < self.min_loss:\n",
    "                count=0\n",
    "                self.min_loss = self.vlad_loss[-1]\n",
    "                self.best = deepcopy(self.model.state_dict())\n",
    "            if count> self.early:\n",
    "                print('Early Stopping\\n')\n",
    "                break\n",
    "            count += 1\n",
    "        end_time = datetime.now()\n",
    "        print(end_time - start_time)\n",
    "        self.model.load_state_dict(self.best)\n",
    "        self.model.train(False)\n",
    "        self.save_model()\n",
    "        self.plot_stats()\n",
    "        return self.test()\n",
    "\n",
    "    \n",
    "    def train(self):\n",
    "        epoch_loss = []\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for text, tag in self.train_loader:\n",
    "            if self.gpu_avail:\n",
    "                text = text.cuda()\n",
    "                tag = tag.cuda()\n",
    "            self.optimizer.zero_grad()\n",
    "            outputs = self.model.forward(text)\n",
    "            loss = self.loss(outputs, tag)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            epoch_loss.append(loss.item())\n",
    "            classification = torch.round(outputs.squeeze())\n",
    "            num_correct = torch.eq(classification, tag.squeeze()).squeeze()\n",
    "            correct += torch.sum(num_correct)\n",
    "            total += (tag.squeeze()).size(0)\n",
    "\n",
    "        self.lr_scheduler.step()\n",
    "        avg = np.array(epoch_loss).mean()\n",
    "        self.train_loss.append(avg)\n",
    "        acc = correct/total\n",
    "        self.train_acc.append(acc)\n",
    "        print(F\"train loss: {avg}\")\n",
    "        print(F\"train accuracy: {acc}\")\n",
    "\n",
    "\n",
    "    def validation(self):\n",
    "        epoch_loss = []\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for text, tag in self.vlad_loader:\n",
    "            if self.gpu_avail:\n",
    "                text = text.cuda()\n",
    "                tag = tag.cuda()\n",
    "            outputs = self.model.forward(text)\n",
    "            loss = self.loss(outputs, tag)\n",
    "            epoch_loss.append(loss.item())\n",
    "            classification = torch.round(outputs.squeeze())\n",
    "            num_correct = torch.eq(classification, tag.squeeze()).squeeze()\n",
    "            correct += torch.sum(num_correct)\n",
    "            total += (tag.squeeze()).size(0)\n",
    "            \n",
    "        acc = correct/total\n",
    "        avg = np.array(epoch_loss).mean()\n",
    "        self.vlad_loss.append(avg)\n",
    "        self.vlad_acc.append(acc)\n",
    "        print(F\"validation loss: {avg}\")\n",
    "        print(F\"validation accuracy: {acc}\")\n",
    "\n",
    "    def test(self):\n",
    "        epoch_loss = []\n",
    "        pred =[]\n",
    "        actual = []\n",
    "        correct = 0 \n",
    "        total = 0\n",
    "        \n",
    "        for text, tag in self.test_loader:\n",
    "            if self.gpu_avail:\n",
    "                text = text.cuda()\n",
    "                tag = tag.cuda()\n",
    "            outputs = self.model.tester(text)\n",
    "            loss = self.loss(outputs, tag)\n",
    "            epoch_loss.append(loss.item())\n",
    "            classification = torch.round(outputs.squeeze())\n",
    "            pred.extend(classification.cpu().detach().numpy())\n",
    "            actual.extend(tag.squeeze().cpu().detach().numpy())\n",
    "            num_correct = torch.eq(classification, tag.squeeze()).squeeze()\n",
    "            correct += torch.sum(num_correct)\n",
    "            total += (tag.squeeze()).size(0)\n",
    "            \n",
    "        acc = correct/total\n",
    "        avg = np.array(epoch_loss).mean()\n",
    "        print(F\"test_loss: {avg}\")\n",
    "        print(F\"test accuracy: {acc}\")\n",
    "        return classification_report(actual, pred, output_dict=True)\n",
    "        \n",
    "    def save_model(self):\n",
    "        model_path = 'latest_model.pt'\n",
    "        model_dict = self.model.state_dict()\n",
    "        state_dict = {'model': model_dict, 'optimizer': self.optimizer.state_dict()}\n",
    "        torch.save(state_dict, model_path)\n",
    "\n",
    "    def plot_stats(self):\n",
    "        e = len(self.train_loss)\n",
    "        x_axis = np.arange(1, e + 1, 1)\n",
    "        plt.figure(figsize=(10,6))\n",
    "        plt.plot(x_axis, self.train_loss, label=\"Training Loss\")\n",
    "        plt.plot(x_axis, self.vlad_loss, label=\"Validation Loss\")\n",
    "        plt.xlabel(\"Epochs\")\n",
    "        plt.legend(loc='best')\n",
    "        plt.title(\"LSTM Sentiment\" + \" Stats Plot\")\n",
    "        plt.savefig(\"Loss_plot.png\")\n",
    "        plt.show()\n",
    "        plt.figure(figsize=(10,6))\n",
    "        plt.plot(x_axis, self.train_loss, label=\"Training Acc\")\n",
    "        plt.plot(x_axis, self.vlad_loss, label=\"Validation Acc\")\n",
    "        plt.xlabel(\"Epochs\")\n",
    "        plt.legend(loc='best')\n",
    "        plt.title(\"LSTM Sentiment\" + \" Stats Plot\")\n",
    "        plt.savefig(\"Accuracy_plot.png\")\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "    # SVM model\n",
    "    def svm_model(self):\n",
    "        vectorizer = TfidfVectorizer(min_df = 5,\n",
    "                                 max_df = 0.8,\n",
    "                                 stop_words = 'english',\n",
    "                                 sublinear_tf = True,\n",
    "                                 use_idf = True)\n",
    "\n",
    "        def convert(x):\n",
    "            keep = []\n",
    "            for i in x:\n",
    "                keep.append(self.vocabulary.idx_word[i])\n",
    "            return ' '.join(keep)\n",
    "\n",
    "        ## Pre-process\n",
    "        svm_train = self.train_data['numerized_tweet'].apply(lambda x: convert(x))\n",
    "        svm_test = self.test_data['numerized_tweet'].apply(lambda x: convert(x))\n",
    "        \n",
    "        #TD-IDF\n",
    "        X_train = vectorizer.fit_transform(svm_train)\n",
    "        X_test = vectorizer.transform(svm_test)\n",
    "        \n",
    "        # Labels\n",
    "        y_train = self.train_data['Toxicity']\n",
    "        y_test = self.test_data['Toxicity']\n",
    "        \n",
    "        \n",
    "        #SVM \n",
    "        classifier = svm.LinearSVC(C = 10**-2)\n",
    "        \n",
    "        \n",
    "        #SVM Train\n",
    "        classifier.fit(X_train, y_train)\n",
    "        \n",
    "        #SVM Test\n",
    "        predictions = classifier.predict(X_test)\n",
    "        \n",
    "        return classification_report(y_test, predictions, output_dict=True)\n",
    "    \n",
    "\n",
    "    # Simple model\n",
    "    def Simple_gusser_Model(self):\n",
    "\n",
    "        np.random.choice([1,0], p =[0.45, 0.55])\n",
    "        \n",
    "        base = []\n",
    "        for i in range(len(self.test_data['Toxicity'])):\n",
    "            base.append(np.random.choice([1,0], p =[0.45, 0.55]))\n",
    "        \n",
    "        return classification_report(self.test_data['Toxicity'], base, output_dict=True)\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\MyJoseph\\Projects\\Scraper\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:67: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.44 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    }
   ],
   "source": [
    "path = \"./data/FinalBalancedDataset.csv\"\n",
    "\n",
    "## Models\n",
    "# put any of the models in teh brackets into the list {svm, simple, lstm}\n",
    "models = ['svm', 'simple', 'lstm']\n",
    "\n",
    "## HyperParms\n",
    "bs = 64\n",
    "embed = 150\n",
    "hdim = 150\n",
    "lr = 0.00005\n",
    "epochs = 30\n",
    "gam= 0.96\n",
    "tr = 0.9\n",
    "ts= 0.1\n",
    "early = 4\n",
    "layers = 4\n",
    "thresh = 5\n",
    "most = 29\n",
    "dropout = 0.44\n",
    "bi = True\n",
    "\n",
    "model = Trainer(path, models, bs, embed, hdim, lr, epochs, gam, tr, ts, early, layers, thresh, most, dropout, bi)\n",
    "model.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "twitterscrape",
   "language": "python",
   "name": "twitterscrape"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
